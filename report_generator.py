from langchain.prompts import PromptTemplate
from rag_engine.llm import load_llm
from rag_engine.embedder import get_embedder
from rag_engine.vector_store import load_vector_db, add_to_vector_db
from rag_engine.search import search_serper
from transformers import AutoTokenizer
from langchain.schema import Document
import uuid
import json
from typing import List
from fastapi import UploadFile
from rag_engine.prompt import get_search_prompt
from rag_engine.loader import load_pdf
import requests
from fastapi import FastAPI, Form
from pydantic import BaseModel
import os

def get_report_prompt():
    template = """
    ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì£¼ì œì— ëŒ€í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    
    ì£¼ì œ: {question}
    ë¬¸ì„œ:
    {context}

    """
    return PromptTemplate.from_template(template)



def generate_report(topic: str, file: UploadFile = None, references: List[str]=None):
    print("ğŸ“Œ 1. í•¨ìˆ˜ ì§„ì… - topic:", topic)
    docs = []
    # 1. PDF ì—…ë¡œë“œ â†’ ì €ì¥
    if file is not None:
        print("ğŸ“Œ 2. íŒŒì¼ ì €ì¥ ì‹œì‘ - filename:", file.filename)
        save_dir = os.path.join(os.getcwd(), "data")
        os.makedirs(save_dir, exist_ok=True)
        filename = f"{uuid.uuid4().hex}_{file.filename}"
        file_path = os.path.join(save_dir, filename)

        with open(file_path, "wb") as f:
            f.write(file.file.read())
        print("âœ… 3. íŒŒì¼ ì €ì¥ ì™„ë£Œ - path:", file_path)

        # 2. PDF â†’ ë¬¸ì„œ ë¶„í• 
        pages = load_pdf(file_path)
        print("âœ… 4. PDF ë¡œë”© ì™„ë£Œ - pages:", len(pages))
        docs = [
            Document(page_content=page.page_content, metadata={"source": filename, "page": i})
            for i, page in enumerate(pages)
        ]

    # 3. ë²¡í„° DBì— ì„ë² ë”©
    embedder = get_embedder()
    vectordb = load_vector_db(embedder)
    if docs:
        add_to_vector_db(docs, vectordb)
        print("âœ… 5. ë²¡í„° DB ì¶”ê°€ ì™„ë£Œ")


    # 4. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰(ë‚´ë¶€ê²€ìƒ‰)
    retriever = vectordb.as_retriever()
    internal_docs = retriever.get_relevant_documents(topic)
    
    # ì™¸ë¶€ ê²€ìƒ‰ (Serper)
    external_docs = search_serper(topic, num_results=3)

    # 6. ë¬¸ì„œ í†µí•© ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±ì„±
    all_docs = internal_docs + external_docs
    context = "\n\n".join([doc.page_content for doc in all_docs])
    print("âœ… 6. ë¬¸ì„œ í†µí•© ì™„ë£Œ - ë¬¸ì„œ ìˆ˜:", len(all_docs))

    # 5. referencesê°€ ìˆë‹¤ë©´ ë¬¸ë§¥ ë’·ë¶€ë¶„ì— ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ë¶™ì´ê¸°
    if references:
        requirements_text = "\n\n[ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­]\n" + "\n".join(references)
        context += requirements_text

    prompt_template = get_search_prompt()
    full_prompt = prompt_template.format(context=context, question=topic)
    print("ğŸ“Œ 7. í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")

    # 7. í† í° ê¸¸ì´ ì œí•œ 
    # tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")
    # tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
    # tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")
    # tokenizer = AutoTokenizer.from_pretrained("NousResearch/Nous-Hermes-2-Mistral-7B-DPO")
    tokenizer = AutoTokenizer.from_pretrained("beomi/llama-2-ko-7b")
    
    
    tokens = tokenizer.encode(full_prompt)
    if len(tokens) > 3000:
        tokens = tokens[:3000]
    trimmed_prompt = tokenizer.decode(tokens)
    print("âœ‚ï¸ 8. í”„ë¡¬í”„íŠ¸ ì˜ë¼ëƒ„ - ê¸¸ì´:", len(tokens))
    
    # 8. LLM í˜¸ì¶œ
    llm = load_llm()
    print("âœ…9.1 LLM ë¡œë”© ì™„ë£Œ")
    # output = llm.invoke(trimmed_prompt)
    # print("ğŸ’¬ 9.2 LLM ì‘ë‹µ ë„ì°©")
    try:
        output = llm.invoke(trimmed_prompt)
        print("ğŸ’¬ 9. LLM ì‘ë‹µ ë„ì°©")
    except Exception as e:
        import traceback
        print("âŒ LLM invoke ì‹¤íŒ¨:")
        traceback.print_exc()
        output = None


    # llm, tokenizer, model = load_llm()

    # max_length = getattr(model.config, "max_position_embeddings", 8192)  # ëª¨ë¸ì—ì„œ ìµœëŒ€ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
    # tokens = tokenizer.encode(full_prompt)

    # if len(tokens) > max_length:
    #     tokens = tokens[:max_length]
    #     full_prompt = tokenizer.decode(tokens)

    # 8. LLM í˜¸ì¶œ
    # print("ğŸ’¬ [ë””ë²„ê¹…] full_prompt ìƒì„± ì™„ë£Œ")
    # print("ğŸ’¬ [ë””ë²„ê¹…] full_prompt ê¸¸ì´:", len(full_prompt))
    # print("ğŸ’¬ [ë””ë²„ê¹…] full_prompt ì• 200ì:\n", full_prompt[:200])

    # output = llm.invoke(full_prompt)
    # print("ğŸ”¥ full_prompt ê¸¸ì´:", len(full_prompt))
    # print("ğŸ”¥ LLM ì¶œë ¥:", output)
    # print("ğŸ”¥ output íƒ€ì…:", type(output))

   


    # 9. ì¶œì²˜ ìˆ˜ì§‘
    sources = [doc.metadata.get("source") for doc in all_docs if "source" in doc.metadata]

    # 10. JSON í˜•íƒœë¡œ ë°˜í™˜
    return {
        "title": f"{topic} ë³´ê³ ì„œ",
        "content": output.strip(),
        "sources": sources
    }



# API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class GenerateReportRequest(BaseModel):
    prompt: str

class GenerateReportResponse(BaseModel):
    title: str
    content: str
    sources: List[str]
    
# FastAPI ë¼ìš°í„°
app = FastAPI(title="AI Report Generator API")

@app.post("/generate_report", response_model=GenerateReportResponse)
async def generate_report_api(prompt: str = Form(...)):
    report = generate_report(prompt)
    # return GenerateReportResponse(prompt=prompt, report=report)
    return GenerateReportResponse(**report)  # âœ… ì–¸íŒ©í•´ì„œ ë”± ë§ê²Œ ì „ë‹¬


if __name__ == "__main__":
    topic = input(" ë³´ê³ ì„œ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    report = generate_report(topic, file=None, references=None)
    print("\nğŸ“„ ë³´ê³ ì„œ ì œëª©:")
    print(report["title"])
    print("\nğŸ“ ë³´ê³ ì„œ ë‚´ìš©:")
    print(report["content"])
    print("\nğŸ”— ì°¸ê³  ì¶œì²˜:")
    for src in report["sources"]:
        print("-", src)



