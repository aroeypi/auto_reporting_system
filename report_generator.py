from langchain.prompts import PromptTemplate
from rag_engine.llm import load_llm
from rag_engine.embedder import get_embedder
from rag_engine.vector_store import load_vector_db, add_to_vector_db
from rag_engine.search import search_serper
from transformers import AutoTokenizer
from langchain.schema import Document
import uuid
import json
from typing import List
from fastapi import UploadFile
from rag_engine.prompt import get_search_prompt
from rag_engine.loader import load_pdf
import requests
from fastapi import FastAPI, Form
from pydantic import BaseModel
import os

def get_report_prompt():
    template = """
    ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì£¼ì œì— ëŒ€í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    
    ì£¼ì œ: {question}
    ë¬¸ì„œ:
    {context}

    """
    return PromptTemplate.from_template(template)



def generate_report(topic: str, file: UploadFile = None, references: List[str]=None):
    docs = []
    # 1. PDF ì—…ë¡œë“œ â†’ ì €ì¥
    if file is not None:
        save_dir = os.path.join(os.getcwd(), "data")
        os.makedirs(save_dir, exist_ok=True)
        filename = f"{uuid.uuid4().hex}_{file.filename}"
        file_path = os.path.join(save_dir, filename)

        with open(file_path, "wb") as f:
            f.write(file.file.read())

        # 2. PDF â†’ ë¬¸ì„œ ë¶„í• 
        pages = load_pdf(file_path)
        docs = [
            Document(page_content=page.page_content, metadata={"source": filename, "page": i})
            for i, page in enumerate(pages)
        ]

    # 3. ë²¡í„° DBì— ì„ë² ë”©
    embedder = get_embedder()
    vectordb = load_vector_db(embedder)
    if docs:
        add_to_vector_db(docs, vectordb)


    # 4. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰(ë‚´ë¶€ê²€ìƒ‰)
    retriever = vectordb.as_retriever()
    internal_docs = retriever.get_relevant_documents(topic)
    
    # ì™¸ë¶€ ê²€ìƒ‰ (Serper)
    external_docs = search_serper(topic, num_results=3)

    # 6. ë¬¸ì„œ í†µí•© ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±ì„±
    all_docs = internal_docs + external_docs
    context = "\n\n".join([doc.page_content for doc in all_docs])

    # 5. referencesê°€ ìˆë‹¤ë©´ ë¬¸ë§¥ ë’·ë¶€ë¶„ì— ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ë¶™ì´ê¸°
    if references:
        requirements_text = "\n\n[ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­]\n" + "\n".join(references)
        context += requirements_text

    prompt_template = get_search_prompt()
    full_prompt = prompt_template.format(context=context, question=topic)


    # 7. í† í° ê¸¸ì´ ì œí•œ 
    tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")
    tokens = tokenizer.encode(full_prompt)
    if len(tokens) > 800:
        tokens = tokens[:800]
    trimmed_prompt = tokenizer.decode(tokens)
    
    # 8. LLM í˜¸ì¶œ
    llm = load_llm()
    output = llm.invoke(trimmed_prompt)

    # 9. ì¶œì²˜ ìˆ˜ì§‘
    sources = [doc.metadata.get("source") for doc in all_docs if "source" in doc.metadata]

    # 10. JSON í˜•íƒœë¡œ ë°˜í™˜
    return {
        "title": f"{topic} ë³´ê³ ì„œ",
        "content": output.strip(),
        "sources": sources
    }



# API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class GenerateReportRequest(BaseModel):
    prompt: str

class GenerateReportResponse(BaseModel):
    title: str
    content: str
    sources: List[str]
    
# FastAPI ë¼ìš°í„°
app = FastAPI(title="AI Report Generator API")

@app.post("/generate_report", response_model=GenerateReportResponse)
async def generate_report_api(prompt: str = Form(...)):
    report = generate_report(prompt)
    return GenerateReportResponse(prompt=prompt, report=report)



if __name__ == "__main__":
    topic = input(" ë³´ê³ ì„œ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    report = generate_report(topic, file=None, references=None)
    print("\nğŸ“„ ë³´ê³ ì„œ ì œëª©:")
    print(report["title"])
    print("\nğŸ“ ë³´ê³ ì„œ ë‚´ìš©:")
    print(report["content"])
    print("\nğŸ”— ì°¸ê³  ì¶œì²˜:")
    for src in report["sources"]:
        print("-", src)



