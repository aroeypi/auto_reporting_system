from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch



def load_llm():
    import os
    os.environ["HF_HOME"] = "D:\huggingface_cache" #usbê²½ë¡œ 
    os.environ["TRANSFORMERS_CACHE"] = "D:/huggingface_cache"
    
    # model_id = "skt/kogpt2-base-v2"
    # model_id = "mistralai/Mistral-7B-Instruct-v0.1"
    # model_id = "HuggingFaceH4/zephyr-7b-beta"
    # model_id = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    model_id = "beomi/llama-2-ko-7b"
    #model_id = "meta-llama/Llama-2-7b-hf"

    print("ğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘:", model_id)

    try:
        # Tokenizer ë¡œë”©
        # tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
        tokenizer = AutoTokenizer.from_pretrained("beomi/llama-2-ko-7b", use_fast=True)
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë”©
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    # Pipeline êµ¬ì„±
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512, #512ê¹Œì§€ ì¶œë ¥ê°€ëŠ¥?
            do_sample=True,
            temperature=0.7,
            top_p=0.95,                     # nucleus sampling ì¡°ì • ê°€ëŠ¥
            repetition_penalty=1.1          # ì¤‘ë³µ ë°©ì§€
        )
        print("âœ… ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
        return HuggingFacePipeline(pipeline=pipe), tokenizer, model
 
    except Exception as e:
        import traceback
        print("âŒ LLM ë¡œë”© ì¤‘ ì˜ˆì™¸ ë°œìƒ!")
        traceback.print_exc()
        raise RuntimeError(f"LLM ë¡œë”© ì‹¤íŒ¨: {e}")